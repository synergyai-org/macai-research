{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일단 시피유로 인퍼런스 ㄱㄱ\n",
    "1. 모델 불러오기 (나중에는 항상 메모리에 올려놓기 ㄱㄱ)\n",
    "2. 데이터 단일 혹은 배치단위로 읽어오기\n",
    "    - 일단 npy\n",
    "    - 나중에 xml을 받아야함.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### json style ###\n",
    "\n",
    "CFG = {\n",
    "    'DEVICE' : 'cpu',\n",
    "    'TARGET_COLS' : [\n",
    "        'ECG_event_3d_AFIB_AFL-keyword_v2_onset',\n",
    "        'ECG_event_3d_CIA-keyword_v2_onset',\n",
    "        'ECG_event_7d_AFIB_AFL-keyword_v2_onset',\n",
    "        'ECG_event_7d_CIA-keyword_v2_onset',\n",
    "        'ECG_event_14d_AFIB_AFL-keyword_v2_onset',\n",
    "        'ECG_event_14d_CIA-keyword_v2_onset',\n",
    "        'ECG_event_30d_AFIB_AFL-keyword_v2_onset',\n",
    "        'ECG_event_30d_CIA-keyword_v2_onset',\n",
    "        'ECG_event_90d_AFIB_AFL-keyword_v2_onset',\n",
    "        'ECG_event_90d_CIA-keyword_v2_onset',\n",
    "        'AFIB_AFL-keyword_v2',\n",
    "        'CIA-keyword_v2'\n",
    "    ],\n",
    "    'MODEL_INFO' : {\n",
    "        'config':{\n",
    "            'embed_dim':768,  \n",
    "            'patch_size':32, \n",
    "            'seq_length':2560,\n",
    "            'in_channels':12, \n",
    "            'encoder':'vit_encoder',\n",
    "            'merge_mode':'projection',  \n",
    "            'num_classes':12,\n",
    "        },        \n",
    "        'model_path_list': [\n",
    "            '/mnt/home/bgk/macai-model-experimental/_kimbg_code/SynAI_v2/outputs/checkpoint/CL_step3-TRN-MacAI_v1_2-0819 copy 21/0fold/Best_AUPRC-Ep_3-Lo_0.082-M0_0.793-M1_0.494.pth',\n",
    "            '/mnt/home/bgk/macai-model-experimental/_kimbg_code/SynAI_v2/outputs/checkpoint/CL_step3-TRN-MacAI_v1_2-0819 copy 21/0fold/Best_AUROC-Ep_7-Lo_0.082-M0_0.800-M1_0.493.pth',\n",
    "            '/mnt/home/bgk/macai-model-experimental/_kimbg_code/SynAI_v2/outputs/checkpoint/CL_step3-TRN-MacAI_v1_2-0819 copy 21/0fold/Best_loss-Ep_5-Lo_0.082-M0_0.797-M1_0.493.pth',\n",
    "        ],\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_architecture import SynAI\n",
    "import torch\n",
    "\n",
    "def model_build(model_info):\n",
    "    model = SynAI.MAE_1D_250409_v3(\n",
    "        seq_length  = model_info['seq_length'],\n",
    "        in_channels = model_info['in_channels'],\n",
    "        patch_size  = model_info['patch_size'],\n",
    "        embed_dim   = model_info['embed_dim'],\n",
    "        merge_mode  = model_info['merge_mode'],  # linear_projection avg add\n",
    "        encoder     = model_info['encoder'],\n",
    "    )\n",
    "    model = SynAI.OnlyEncoderForFT_250409(\n",
    "        model,\n",
    "        num_classes = model_info['num_classes'],\n",
    "        embed_dim = model_info['embed_dim'],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def load_models(model_path_list, model_info, device='cpu'):\n",
    "    models = []\n",
    "    for model_path in model_path_list:\n",
    "        checkpoint = torch.load(model_path, weights_only=True)\n",
    "        model = model_build(model_info).to(device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        models.append({'model_path': model_path, 'model': model})\n",
    "    return models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syai-research/.local/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_length 2560, in_channels 12, patch_size 32, embed_dim 768, token_len 80, \n",
      "seq_length 2560, in_channels 12, patch_size 32, embed_dim 768, token_len 80, \n",
      "seq_length 2560, in_channels 12, patch_size 32, embed_dim 768, token_len 80, \n"
     ]
    }
   ],
   "source": [
    "models = load_models(CFG['MODEL_INFO']['model_path_list'], \n",
    "                     CFG['MODEL_INFO']['config'], device=CFG['DEVICE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from processing import transforms, dataset\n",
    "\n",
    "transform_pipeline = {}\n",
    "transform_pipeline['val'] = transforms.TransformPipeline([\n",
    "    transforms.NormalizeECG(method=\"tanh\", scope=\"lead-wise\", scale=1),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1563535/1969612414.py:3: DtypeWarning: Columns (4,14) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(df_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/home/Datasets_processed/EKG_Latest/signal-bpf0540_or3/10302870_2023-10-17_2023101716092374_2023101716095795.npy',\n",
       " '/home/Datasets_processed/EKG_Latest/signal-bpf0540_or3/10478425_2024-02-28_2024022823332301_2024022823344711.npy']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_path = '/mnt/home/Datasets_processed/EKG_Latest/processed_metadata_v6-sv2_subset_2_holdout.csv'\n",
    "df = pd.read_csv(df_path)\n",
    "tmp_npy_path = df['bpf0540_or3-npy_path'][:2]\n",
    "tmp_npy_path.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bandpass_info = {\n",
    "    \"lowcut\": 0.5,\n",
    "    \"highcut\": 40.0,\n",
    "    \"order\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "배치 텐서 크기: torch.Size([2, 12, 2560])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from processing import transforms, dataset\n",
    "\n",
    "# 배치 데이터를 저장할 리스트\n",
    "batch_ecg = []\n",
    "\n",
    "for i in range(len(tmp_npy_path)):\n",
    "    # ECG 데이터 로드 및 전처리\n",
    "    tmp_ecg = np.load('/mnt' + tmp_npy_path[i])\n",
    "    tmp_ecg = dataset.ECGDataset_v2.preprocess_ecg(\n",
    "        x               = tmp_ecg, \n",
    "        scaling_factor  = None,\n",
    "        do_bandpass     = bandpass_info,\n",
    "        original_fs     = 500,\n",
    "        target_fs       = 256,\n",
    "        do_normalize    = False,\n",
    "        transforms      = transform_pipeline['val']\n",
    "    )\n",
    "    # 배치에 추가\n",
    "    batch_ecg.append(tmp_ecg)\n",
    "\n",
    "# 리스트를 PyTorch 텐서로 변환 (배치 크기, 채널, 시퀀스 길이)\n",
    "batch_ecg_tensor = torch.tensor(np.stack(batch_ecg), dtype=torch.float32)\n",
    "\n",
    "# 결과 확인\n",
    "print(\"배치 텐서 크기:\", batch_ecg_tensor.shape)  # 예: (배치 크기, 채널, 시퀀스 길이)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Dict, Optional, List\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def inference_tensor(\n",
    "    model: torch.nn.Module,\n",
    "    input_tensor: torch.Tensor,\n",
    "    activation: Optional[str] = 'sigmoid',\n",
    "    to_cpu: bool = True\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    단일 샘플 또는 배치 텐서에 대해 추론 결과 반환\n",
    "    \"\"\"\n",
    "    original_input = input_tensor.clone().detach()\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    inference_input = input_tensor.to(device)\n",
    "    is_batch = inference_input.dim() >= 3\n",
    "    if not is_batch:\n",
    "        inference_input = inference_input.unsqueeze(0)\n",
    "    with torch.cuda.amp.autocast(enabled=(device.type == 'cuda')):\n",
    "        logits = model(inference_input)\n",
    "    if not is_batch:\n",
    "        logits = logits.squeeze(0)\n",
    "    probs = None\n",
    "    if activation == 'sigmoid':\n",
    "        probs = torch.sigmoid(logits)\n",
    "    elif activation == 'softmax':\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "    if to_cpu:\n",
    "        logits = logits.cpu()\n",
    "        if probs is not None:\n",
    "            probs = probs.cpu()\n",
    "    result = {\n",
    "        \"input\": original_input,\n",
    "        \"logits\": logits\n",
    "    }\n",
    "    if probs is not None:\n",
    "        result[\"probs\"] = probs\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': tensor([[[-2.9693e-03, -7.0048e-03, -8.8557e-03,  ...,  1.9844e-03,\n",
       "            1.3033e-03,  1.9885e-03],\n",
       "          [-2.0600e-03, -4.3306e-03, -4.3705e-03,  ...,  3.5940e-03,\n",
       "            2.6778e-03,  2.8508e-03],\n",
       "          [ 1.7023e-03,  5.2238e-03,  8.3791e-03,  ...,  1.5170e-03,\n",
       "            1.3776e-03,  1.0135e-03],\n",
       "          ...,\n",
       "          [-8.2133e-03, -1.0171e-02, -1.4388e-02,  ..., -1.4268e-02,\n",
       "           -1.3296e-02, -1.3271e-02],\n",
       "          [ 4.5172e-04,  1.5961e-02,  2.3923e-02,  ..., -1.3827e-02,\n",
       "           -1.2010e-02, -1.2967e-02],\n",
       "          [ 6.6286e-03,  3.4782e-02,  5.3068e-02,  ..., -7.7860e-03,\n",
       "           -6.4454e-03, -8.2739e-03]],\n",
       " \n",
       "         [[-3.4640e-03,  3.5442e-02,  7.9031e-02,  ..., -1.8068e-03,\n",
       "           -1.8399e-03, -1.6192e-03],\n",
       "          [ 2.9238e-02,  6.0985e-02,  7.3498e-02,  ..., -2.2764e-03,\n",
       "            4.3108e-04, -3.7820e-03],\n",
       "          [ 3.2890e-02,  2.5221e-02, -6.6650e-03,  ..., -9.2264e-06,\n",
       "            2.5665e-03, -1.9017e-03],\n",
       "          ...,\n",
       "          [ 8.1878e-04,  9.3865e-04,  2.8620e-03,  ...,  9.4936e-04,\n",
       "            9.6728e-04,  9.7748e-04],\n",
       "          [ 8.6592e-03,  1.9849e-02,  2.7903e-02,  ...,  7.4578e-04,\n",
       "            1.3563e-03,  1.0665e-04],\n",
       "          [ 1.1341e-02,  2.5154e-02,  3.3542e-02,  ...,  1.9480e-04,\n",
       "            1.0213e-03, -8.7918e-04]]]),\n",
       " 'logits': tensor([[-2.6173, -1.7039, -2.6186, -1.6557, -2.5558, -1.5598, -2.5421, -1.5749,\n",
       "          -2.5091, -1.5497, -2.9607, -2.7157],\n",
       "         [-1.4546,  1.2509, -1.3559,  1.4660, -1.3906,  1.5366, -1.2304,  1.4362,\n",
       "          -1.1279,  1.5210, -2.6559,  1.6034]]),\n",
       " 'probs': tensor([[0.0680, 0.1540, 0.0680, 0.1603, 0.0720, 0.1737, 0.0730, 0.1715, 0.0752,\n",
       "          0.1751, 0.0492, 0.0621],\n",
       "         [0.1893, 0.7775, 0.2049, 0.8124, 0.1993, 0.8230, 0.2261, 0.8079, 0.2445,\n",
       "          0.8207, 0.0656, 0.8325]])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = inference_tensor(\n",
    "    models[0]['model'], batch_ecg_tensor)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['probs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_idx = CFG['TARGET_COLS'].index('ECG_event_14d_AFIB_AFL-keyword_v2_onset')\n",
    "cia_idx = CFG['TARGET_COLS'].index('ECG_event_14d_CIA-keyword_v2_onset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0720, 0.1993])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['probs'][:, af_idx]\n",
    "result['probs'][:, cia_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "af_cal_model = joblib.load('/mnt/home/bgk/macai-engine-v1/trained_models/v1.2.1/calibration_model-AF.pkl')\n",
    "af_prob_cal = af_cal_model.predict_proba(result['probs'][:, af_idx].reshape(-1, 1))[:, 1]\n",
    "cia_cal_model = joblib.load('/mnt/home/bgk/macai-engine-v1/trained_models/v1.2.1/calibration_model-CIA.pkl')\n",
    "cia_prob_cal = cia_cal_model.predict_proba(result['probs'][:, cia_idx].reshape(-1, 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_high_g_thr = 0.18594001505695362\n",
    "af_inter_g_thr = 0.09283757956391994\n",
    "cia_high_g_thr = 0.6134156782179706\n",
    "cia_inter_g_thr = 0.39689483565475614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_risk(\n",
    "    probs:np.array, high_thr:float, inter_thr:float, method:str=None):\n",
    "    '''\n",
    "        probs: 위험도 확률 리스트\n",
    "        high_thr: 고위험군 기준 임계값 (상위 10%)\n",
    "        inter_thr: 중간위험군 기준 임계값 (상위 10% ~ 상위 30%)\n",
    "    '''\n",
    "    results = []    \n",
    "    if method == 'mean':\n",
    "        probs = probs.mean()\n",
    "        if probs >= high_thr:\n",
    "            return [probs, \"High\"]\n",
    "        elif probs >= inter_thr:\n",
    "            return [probs, \"Intermediate\"]\n",
    "        else:\n",
    "            return [probs, \"Low\"]\n",
    "    else:\n",
    "        for prob in probs:\n",
    "            if prob >= high_thr:\n",
    "                results.append([prob, \"High\"])\n",
    "            elif prob >= inter_thr:\n",
    "                results.append([prob, \"Intermediate\"])\n",
    "            else:\n",
    "                results.append([prob, \"Low\"])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.08155256296124591, 'Low'], [0.5112336937366079, 'Intermediate'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af_risk = classify_risk(af_prob_cal, af_high_g_thr, af_inter_g_thr, method='mean')\n",
    "cia_risk = classify_risk(cia_prob_cal, cia_high_g_thr, cia_inter_g_thr, method='mean')\n",
    "af_risk, cia_risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## yaml test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MODEL_INFO': {'DEVICE': 'cpu', 'CONFIG': {'embed_dim': 768, 'patch_size': 32, 'seq_length': 2560, 'in_channels': 12, 'encoder': 'vit_encoder', 'merge_mode': 'projection', 'num_classes': 12}, 'MODEL_PATHS': ['/mnt/home/bgk/macai-model-experimental/_kimbg_code/SynAI_v2/outputs/checkpoint/CL_step3-TRN-MacAI_v1_2-0819 copy 21/0fold/Best_AUPRC-Ep_3-Lo_0.082-M0_0.793-M1_0.494.pth', '/mnt/home/bgk/macai-model-experimental/_kimbg_code/SynAI_v2/outputs/checkpoint/CL_step3-TRN-MacAI_v1_2-0819 copy 21/0fold/Best_AUROC-Ep_7-Lo_0.082-M0_0.800-M1_0.493.pth', '/mnt/home/bgk/macai-model-experimental/_kimbg_code/SynAI_v2/outputs/checkpoint/CL_step3-TRN-MacAI_v1_2-0819 copy 21/0fold/Best_loss-Ep_5-Lo_0.082-M0_0.797-M1_0.493.pth'], 'AF_CAL_MODEL_PATH': '/mnt/home/bgk/macai-engine-v1/trained_models/v1.2.1/calibration_model-AF.pkl', 'CIA_CAL_MODEL_PATH': '/mnt/home/bgk/macai-engine-v1/trained_models/v1.2.1/calibration_model-CIA.pkl', 'RISK_THRESHOLD': {'af_high_g_thr': 0.18594001505695362, 'af_inter_g_thr': 0.09283757956391994, 'cia_high_g_thr': 0.6134156782179706, 'cia_inter_g_thr': 0.39689483565475614}}, 'DATA_INFO': {'OUTPUT_CLS_INFO': {'cls_names': ['ECG_event_3d_AFIB_AFL-keyword_v2_onset', 'ECG_event_3d_CIA-keyword_v2_onset', 'ECG_event_7d_AFIB_AFL-keyword_v2_onset', 'ECG_event_7d_CIA-keyword_v2_onset', 'ECG_event_14d_AFIB_AFL-keyword_v2_onset', 'ECG_event_14d_CIA-keyword_v2_onset', 'ECG_event_30d_AFIB_AFL-keyword_v2_onset', 'ECG_event_30d_CIA-keyword_v2_onset', 'ECG_event_90d_AFIB_AFL-keyword_v2_onset', 'ECG_event_90d_CIA-keyword_v2_onset', 'AFIB_AFL-keyword_v2', 'CIA-keyword_v2'], 'target_cls': {'AF': 'ECG_event_14d_AFIB_AFL-keyword_v2_onset', 'CIA': 'ECG_event_14d_CIA-keyword_v2_onset'}}, 'ECG_PATHS': ['/mnt/home/Datasets_processed/EKG_Latest/signal-bpf0540_or3/10302870_2023-10-17_2023101716092374_2023101716095795.npy', '/mnt/home/Datasets_processed/EKG_Latest/signal-bpf0540_or3/10203160_2023-10-04_2023100411441625_2023101110031840.npy'], 'PREPROCESS': {'scaling_factor': None, 'do_bandpass': None, 'original_fs': 500, 'target_fs': 256, 'do_normalize': False}}}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "config_path = '/mnt/home/bgk/macai-engine-v1/trained_models/v1.2.1/config.yaml'\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/syai-research/.local/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq_length 2560, in_channels 12, patch_size 32, embed_dim 768, token_len 80, \n",
      "seq_length 2560, in_channels 12, patch_size 32, embed_dim 768, token_len 80, \n",
      "seq_length 2560, in_channels 12, patch_size 32, embed_dim 768, token_len 80, \n"
     ]
    }
   ],
   "source": [
    "from processing import infer\n",
    "models = infer.load_models(config['MODEL_INFO']['MODEL_PATHS'], \n",
    "                           config['MODEL_INFO']['CONFIG'], device=config['MODEL_INFO']['DEVICE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "af_cal_model = joblib.load(config['MODEL_INFO']['AF_CAL_MODEL_PATH'])\n",
    "cia_cal_model = joblib.load(config['MODEL_INFO']['CIA_CAL_MODEL_PATH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12, 2560])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from processing import transforms, dataset, infer\n",
    "\n",
    "transform_pipeline = {}\n",
    "transform_pipeline['val'] = transforms.TransformPipeline([\n",
    "    transforms.NormalizeECG(method=\"tanh\", scope=\"lead-wise\", scale=1),\n",
    "])\n",
    "\n",
    "\n",
    "### 임시 ###\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "batch_ecg = []\n",
    "for tmp_npy_path in config['DATA_INFO']['ECG_PATHS']:\n",
    "    tmp_ecg = np.load(tmp_npy_path)\n",
    "    tmp_ecg = dataset.ECGDataset_v2.preprocess_ecg(\n",
    "        x               = tmp_ecg, \n",
    "        scaling_factor  = config['DATA_INFO']['PREPROCESS']['scaling_factor'],\n",
    "        do_bandpass     = config['DATA_INFO']['PREPROCESS']['do_bandpass'],\n",
    "        original_fs     = config['DATA_INFO']['PREPROCESS']['original_fs'],\n",
    "        target_fs       = config['DATA_INFO']['PREPROCESS']['target_fs'],\n",
    "        do_normalize    = config['DATA_INFO']['PREPROCESS']['do_normalize'],\n",
    "        transforms      = transform_pipeline['val']\n",
    "    )\n",
    "    batch_ecg.append(tmp_ecg)\n",
    "batch_ecg_tensor = torch.tensor(np.stack(batch_ecg), dtype=torch.float32)\n",
    "print(batch_ecg_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 확률값: (2, 12)\n"
     ]
    }
   ],
   "source": [
    "### 임시 ###\n",
    "results = []\n",
    "for i in range(len(models)):\n",
    "    result = infer.inference_tensor(models[i]['model'], batch_ecg_tensor)\n",
    "    results.append(result)\n",
    "probs_list = [result['probs'] for result in results]  # results는 각 모델의 결과 리스트\n",
    "stacked_probs = torch.stack(probs_list, dim=0)  # (모델 개수, 배치 크기, 클래스 개수)\n",
    "mean_probs = stacked_probs.mean(dim=0)  # 모델 차원(0)을 기준으로 평균 계산\n",
    "mean_probs = np.array(mean_probs)\n",
    "print(\"평균 확률값:\", mean_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "af_idx = config['DATA_INFO']['OUTPUT_CLS_INFO']['cls_names'].index(config['DATA_INFO']['OUTPUT_CLS_INFO']['target_cls']['AF'])\n",
    "cia_idx = config['DATA_INFO']['OUTPUT_CLS_INFO']['cls_names'].index(config['DATA_INFO']['OUTPUT_CLS_INFO']['target_cls']['CIA'])\n",
    "af_prob_cal = af_cal_model.predict_proba(mean_probs[:, af_idx].reshape(-1, 1))[:, 1]\n",
    "cia_prob_cal = cia_cal_model.predict_proba(mean_probs[:, cia_idx].reshape(-1, 1))[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.09766305002407535, 'Intermediate'], [0.5498921614999946, 'Intermediate'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "af_risk = infer.classify_risk(\n",
    "    probs = af_prob_cal, \n",
    "    high_thr = config['MODEL_INFO']['RISK_THRESHOLD']['af_high_g_thr'], \n",
    "    inter_thr = config['MODEL_INFO']['RISK_THRESHOLD']['af_inter_g_thr'], \n",
    "    method = 'mean'\n",
    ")\n",
    "cia_risk = infer.classify_risk(\n",
    "    probs = cia_prob_cal, \n",
    "    high_thr = config['MODEL_INFO']['RISK_THRESHOLD']['cia_high_g_thr'], \n",
    "    inter_thr = config['MODEL_INFO']['RISK_THRESHOLD']['cia_inter_g_thr'], \n",
    "    method = 'mean'\n",
    ")\n",
    "af_risk, cia_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "macai-trn-v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
